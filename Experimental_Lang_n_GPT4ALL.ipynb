{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35cbfe2-7aff-4734-b237-ef0230f66563",
   "metadata": {},
   "source": [
    "# Experimental - Langchain + GPT4ALL\n",
    "Ref: \n",
    "- [Langchain Integration - GPT4All](https://python.langchain.com/en/latest/modules/models/llms/integrations/gpt4all.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf341628-0750-422a-912d-3f3ccbebf6a7",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03c3cc3-da73-400b-9608-0d9c40cc8152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afcc8f1-c0bf-4103-988c-b0ad9dfa6fac",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e839f-ad0b-42e6-985d-d36bc5abc38d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_local_path = './models/'\n",
    "model_name = 'ggml-gpt4all-j-v1.3-groovy.bin'\n",
    "\n",
    "model_full_local_path = model_local_path + model_name\n",
    "\n",
    "local_path = './models/ggml-gpt4all-j-v1.3-groovy.bin' \n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
    "# If you want to use a custom model add the backend parameter\n",
    "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "llm = GPT4All(model=local_path, backend='gptj', callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ec3af-5eb4-468e-8080-73391f61cf36",
   "metadata": {},
   "source": [
    "## Prepare prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83577f6-aef0-4740-9262-cdbb81bdbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb66a6-12b7-4ee6-a368-4c45d46d7184",
   "metadata": {},
   "source": [
    "## Evalmuation & Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc1844-5c8a-48cc-bf92-d0b33a4602d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"How Red Black Tree algorithm works?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4064957-bc0c-4899-8840-1df51b7c2201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6eef78-e992-44b5-873d-10757273f8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cabff2-6807-4aae-8a47-08fd50ec1244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
